package dataengine.pipeline.transformation;

import dataengine.pipeline.Data2Transformation;
import dataengine.pipeline.DataTransformation;
import dataengine.pipeline.sql.SparkSqlUnresolvedRelationResolver;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import javax.annotation.Nonnull;

public class SqlTransformations {

    public static <S> DataTransformation<S, Row> sql(@Nonnull String sourceName, @Nonnull String sql) {
        return s -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder().plan(sourceName, s.logicalPlan()).build();
            return resolver.resolveAsDataset(s.sparkSession(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @return outcome of the merge operation
     */
    public static <S1, S2> Data2Transformation<S1, S2, Row> sqlMerge(@Nonnull String sourceName1,
                                                                     @Nonnull String sourceName2,
                                                                     @Nonnull String sql) {
        return (s1, s2) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    // start of code generated by a utility

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3> Merge.Data3Transformation<S1, S2, S3, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sql) {
        return (s1, s2, s3) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4> Merge.Data4Transformation<S1, S2, S3, S4, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sql) {
        return (s1, s2, s3, s4) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sourceName5 name of input dataset #5
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @param <S5>        type of the input dataset #5
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5> Merge.Data5Transformation<S1, S2, S3, S4, S5, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sourceName5 name of input dataset #5
     * @param sourceName6 name of input dataset #6
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @param <S5>        type of the input dataset #5
     * @param <S6>        type of the input dataset #6
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5, S6> Merge.Data6Transformation<S1, S2, S3, S4, S5, S6, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sourceName6,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5, s6) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .plan(sourceName6, s6.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sourceName5 name of input dataset #5
     * @param sourceName6 name of input dataset #6
     * @param sourceName7 name of input dataset #7
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @param <S5>        type of the input dataset #5
     * @param <S6>        type of the input dataset #6
     * @param <S7>        type of the input dataset #7
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5, S6, S7> Merge.Data7Transformation<S1, S2, S3, S4, S5, S6, S7, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sourceName6,
            @Nonnull String sourceName7,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5, s6, s7) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .plan(sourceName6, s6.logicalPlan())
                    .plan(sourceName7, s7.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sourceName5 name of input dataset #5
     * @param sourceName6 name of input dataset #6
     * @param sourceName7 name of input dataset #7
     * @param sourceName8 name of input dataset #8
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @param <S5>        type of the input dataset #5
     * @param <S6>        type of the input dataset #6
     * @param <S7>        type of the input dataset #7
     * @param <S8>        type of the input dataset #8
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5, S6, S7, S8> Merge.Data8Transformation<S1, S2, S3, S4, S5, S6, S7, S8, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sourceName6,
            @Nonnull String sourceName7,
            @Nonnull String sourceName8,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5, s6, s7, s8) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .plan(sourceName6, s6.logicalPlan())
                    .plan(sourceName7, s7.logicalPlan())
                    .plan(sourceName8, s8.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1 name of input dataset #1
     * @param sourceName2 name of input dataset #2
     * @param sourceName3 name of input dataset #3
     * @param sourceName4 name of input dataset #4
     * @param sourceName5 name of input dataset #5
     * @param sourceName6 name of input dataset #6
     * @param sourceName7 name of input dataset #7
     * @param sourceName8 name of input dataset #8
     * @param sourceName9 name of input dataset #9
     * @param sql         sql transformation
     * @param <S1>        type of the input dataset #1
     * @param <S2>        type of the input dataset #2
     * @param <S3>        type of the input dataset #3
     * @param <S4>        type of the input dataset #4
     * @param <S5>        type of the input dataset #5
     * @param <S6>        type of the input dataset #6
     * @param <S7>        type of the input dataset #7
     * @param <S8>        type of the input dataset #8
     * @param <S9>        type of the input dataset #9
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5, S6, S7, S8, S9> Merge.Data9Transformation<S1, S2, S3, S4, S5, S6, S7, S8, S9, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sourceName6,
            @Nonnull String sourceName7,
            @Nonnull String sourceName8,
            @Nonnull String sourceName9,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5, s6, s7, s8, s9) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .plan(sourceName6, s6.logicalPlan())
                    .plan(sourceName7, s7.logicalPlan())
                    .plan(sourceName8, s8.logicalPlan())
                    .plan(sourceName9, s9.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }

    /**
     * Applies a sql merge transformation to all the input DataPipes.
     *
     * @param sourceName1  name of input dataset #1
     * @param sourceName2  name of input dataset #2
     * @param sourceName3  name of input dataset #3
     * @param sourceName4  name of input dataset #4
     * @param sourceName5  name of input dataset #5
     * @param sourceName6  name of input dataset #6
     * @param sourceName7  name of input dataset #7
     * @param sourceName8  name of input dataset #8
     * @param sourceName9  name of input dataset #9
     * @param sourceName10 name of input dataset #10
     * @param sql          sql transformation
     * @param <S1>         type of the input dataset #1
     * @param <S2>         type of the input dataset #2
     * @param <S3>         type of the input dataset #3
     * @param <S4>         type of the input dataset #4
     * @param <S5>         type of the input dataset #5
     * @param <S6>         type of the input dataset #6
     * @param <S7>         type of the input dataset #7
     * @param <S8>         type of the input dataset #8
     * @param <S9>         type of the input dataset #9
     * @param <S10>        type of the input dataset #10
     * @return outcome of the merge operation
     */
    public static <S1, S2, S3, S4, S5, S6, S7, S8, S9, S10> Merge.Data10Transformation<S1, S2, S3, S4, S5, S6, S7, S8, S9, S10, Row> sqlMerge(
            @Nonnull String sourceName1,
            @Nonnull String sourceName2,
            @Nonnull String sourceName3,
            @Nonnull String sourceName4,
            @Nonnull String sourceName5,
            @Nonnull String sourceName6,
            @Nonnull String sourceName7,
            @Nonnull String sourceName8,
            @Nonnull String sourceName9,
            @Nonnull String sourceName10,
            @Nonnull String sql) {
        return (s1, s2, s3, s4, s5, s6, s7, s8, s9, s10) -> {
            SparkSqlUnresolvedRelationResolver resolver = SparkSqlUnresolvedRelationResolver.builder()
                    .plan(sourceName1, s1.logicalPlan())
                    .plan(sourceName2, s2.logicalPlan())
                    .plan(sourceName3, s3.logicalPlan())
                    .plan(sourceName4, s4.logicalPlan())
                    .plan(sourceName5, s5.logicalPlan())
                    .plan(sourceName6, s6.logicalPlan())
                    .plan(sourceName7, s7.logicalPlan())
                    .plan(sourceName8, s8.logicalPlan())
                    .plan(sourceName9, s9.logicalPlan())
                    .plan(sourceName10, s10.logicalPlan())
                    .build();
            return resolver.resolveAsDataset(SparkSession.active(), sql);
        };
    }
}
